{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d553f2d-5577-4dbc-a8f7-2d1198c38ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = 092e1c15-d10e-408c-9771-402cb70d901c, runId = 09e3817b-a974-4351-9dd0-1b79511c5b57] terminated with exception: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (1304e2995f76 executor driver): org.apache.kafka.common.KafkaException: Failed to construct kafka consumer\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:823)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:665)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:613)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.createConsumer(KafkaDataConsumer.scala:124)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.<init>(KafkaDataConsumer.scala:61)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumerPool$ObjectFactory.create(InternalKafkaConsumerPool.scala:207)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumerPool$ObjectFactory.create(InternalKafkaConsumerPool.scala:202)\n\tat org.apache.commons.pool2.BaseKeyedPooledObjectFactory.makeObject(BaseKeyedPooledObjectFactory.java:82)\n\tat org.apache.commons.pool2.impl.GenericKeyedObjectPool.create(GenericKeyedObjectPool.java:780)\n\tat org.apache.commons.pool2.impl.GenericKeyedObjectPool.borrowObject(GenericKeyedObjectPool.java:439)\n\tat org.apache.commons.pool2.impl.GenericKeyedObjectPool.borrowObject(GenericKeyedObjectPool.java:350)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumerPool.borrowObject(InternalKafkaConsumerPool.scala:85)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.retrieveConsumer(KafkaDataConsumer.scala:573)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getOrRetrieveConsumer(KafkaDataConsumer.scala:558)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:294)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.get(KafkaDataConsumer.scala:290)\n\tat org.apache.spark.sql.kafka010.KafkaBatchPartitionReader.next(KafkaBatchPartitionReader.scala:64)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:156)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n\tat scala.Option.exists(Option.scala:376)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:97)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:82)\n\tat org.elasticsearch.spark.sql.streaming.EsStreamQueryWriter.run(EsStreamQueryWriter.scala:62)\n\tat org.elasticsearch.spark.sql.streaming.EsSparkSqlStreamingSink.$anonfun$addBatch$5(EsSparkSqlStreamingSink.scala:72)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:731)\n\t... 41 more\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 87\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Kafka stream visible côté console (debug)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m raw_df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 87\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = 092e1c15-d10e-408c-9771-402cb70d901c, runId = 09e3817b-a974-4351-9dd0-1b79511c5b57] terminated with exception: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (1304e2995f76 executor driver): org.apache.kafka.common.KafkaException: Failed to construct kafka consumer\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:823)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:665)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:613)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.createConsumer(KafkaDataConsumer.scala:124)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.<init>(KafkaDataConsumer.scala:61)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumerPool$ObjectFactory.create(InternalKafkaConsumerPool.scala:207)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumerPool$ObjectFactory.create(InternalKafkaConsumerPool.scala:202)\n\tat org.apache.commons.pool2.BaseKeyedPooledObjectFactory.makeObject(BaseKeyedPooledObjectFactory.java:82)\n\tat org.apache.commons.pool2.impl.GenericKeyedObjectPool.create(GenericKeyedObjectPool.java:780)\n\tat org.apache.commons.pool2.impl.GenericKeyedObjectPool.borrowObject(GenericKeyedObjectPool.java:439)\n\tat org.apache.commons.pool2.impl.GenericKeyedObjectPool.borrowObject(GenericKeyedObjectPool.java:350)\n\tat org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumerPool.borrowObject(InternalKafkaConsumerPool.scala:85)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.retrieveConsumer(KafkaDataConsumer.scala:573)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getOrRetrieveConsumer(KafkaDataConsumer.scala:558)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:294)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)\n\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.get(KafkaDataConsumer.scala:290)\n\tat org.apache.spark.sql.kafka010.KafkaBatchPartitionReader.next(KafkaBatchPartitionReader.scala:64)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:156)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n\tat scala.Option.exists(Option.scala:376)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:97)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:82)\n\tat org.elasticsearch.spark.sql.streaming.EsStreamQueryWriter.run(EsStreamQueryWriter.scala:62)\n\tat org.elasticsearch.spark.sql.streaming.EsSparkSqlStreamingSink.$anonfun$addBatch$5(EsSparkSqlStreamingSink.scala:72)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:731)\n\t... 41 more\n\nDriver stacktrace:"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, when, window, expr\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Kafka\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,\"\n",
    "            \"org.elasticsearch:elasticsearch-spark-30_2.12:8.12.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schéma des données JSON\n",
    "schema = StructType() \\\n",
    "    .add(\"device_id\", StringType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"humidity\", DoubleType()) \\\n",
    "    .add(\"pressure\", DoubleType()) \\\n",
    "    .add(\"vibration\", DoubleType()) \\\n",
    "    .add(\"latitude\", DoubleType()) \\\n",
    "    .add(\"longitude\", DoubleType()) \\\n",
    "    .add(\"timestamp\", StringType())\n",
    "\n",
    "# Lire depuis Kafka\n",
    "raw_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
    "    .option(\"subscribe\", \"iot_raw_data\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Extraire le JSON\n",
    "parsed_df = raw_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Conversion du timestamp\n",
    "parsed_df = parsed_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Nettoyage des valeurs manquantes\n",
    "clean_df = parsed_df.dropna()\n",
    "\n",
    "# Ajout des règles métiers\n",
    "alert_df = clean_df.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"temperature\") > 50, \"Alerte\")\n",
    "    .when((col(\"humidity\") < 10) & (col(\"temperature\") > 40), \"Critique\")\n",
    "    .when((col(\"vibration\") > 5) & ((col(\"pressure\") < 950) | (col(\"pressure\") > 1050)), \"Alerte\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Capteurs inactifs (si aucune donnée depuis >1h) => traitement par une autre logique (batch/join avec état)\n",
    "\n",
    "# Exemple de corrélation régionale : moyenne température par région (arrondi lat/lon à 0.1°)\n",
    "regional_alerts = alert_df \\\n",
    "    .withColumn(\"region_lat\", expr(\"round(latitude, 1)\")) \\\n",
    "    .withColumn(\"region_lon\", expr(\"round(longitude, 1)\")) \\\n",
    "    .groupBy(window(\"timestamp\", \"10 minutes\"), \"region_lat\", \"region_lon\") \\\n",
    "    .avg(\"temperature\", \"humidity\") \\\n",
    "    .withColumnRenamed(\"avg(temperature)\", \"avg_temp\") \\\n",
    "    .withColumnRenamed(\"avg(humidity)\", \"avg_humidity\") \\\n",
    "    .withColumn(\"regional_status\", when(col(\"avg_temp\") > 45, \"Alerte regionale\").otherwise(\"OK\"))\n",
    "\n",
    "# Écriture dans Elasticsearch\n",
    "query = alert_df.writeStream \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_iot\") \\\n",
    "    .option(\"es.nodes\", \"es01\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "    .option(\"es.net.ssl\", \"true\") \\\n",
    "    .option(\"es.net.ssl.cert.allow.self.signed\", \"true\") \\\n",
    "    .option(\"es.net.http.auth.user\", \"elastic\") \\\n",
    "    .option(\"es.net.http.auth.pass\", \"Eselpil2\") \\\n",
    "    .option(\"es.resource\", \"agritech_iot_data_enriched\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Kafka stream visible côté console (debug)\n",
    "raw_df.selectExpr(\"CAST(value AS STRING)\").writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c90991-b2b6-45d4-a966-775181e9c3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bbc8b-4030-435f-b089-ea280883232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update && apt install -y netcat\n",
    "!nc -zv broker 9092\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7dff29-aecd-447d-a64b-6e71b73d4b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'es01'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get(\"https://es01:9200/\", auth=(\"elastic\", \"Eselpil2\"), verify=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d950c94-5ea9-47d6-85d8-7043bd8322aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: curl: command not found\n"
     ]
    }
   ],
   "source": [
    "!curl -u elastic:Eselpil2 https://es01:9200 -k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ed25f6-c9cb-418e-94eb-84b434727fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c2f71-8b04-4bf4-b498-55d646ef180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, when, window, expr\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Kafka\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,\"\n",
    "            \"org.elasticsearch:elasticsearch-spark-30_2.12:8.12.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schéma des données JSON\n",
    "schema = StructType() \\\n",
    "    .add(\"device_id\", StringType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"humidity\", DoubleType()) \\\n",
    "    .add(\"pressure\", DoubleType()) \\\n",
    "    .add(\"vibration\", DoubleType()) \\\n",
    "    .add(\"latitude\", DoubleType()) \\\n",
    "    .add(\"longitude\", DoubleType()) \\\n",
    "    .add(\"timestamp\", StringType())\n",
    "\n",
    "# Lire depuis Kafka\n",
    "raw_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
    "    .option(\"subscribe\", \"iot_raw_data\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Extraire le JSON\n",
    "parsed_df = raw_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Conversion du timestamp\n",
    "parsed_df = parsed_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Nettoyage des valeurs manquantes\n",
    "clean_df = parsed_df.dropna()\n",
    "\n",
    "# Ajout des règles métiers\n",
    "alert_df = clean_df.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"temperature\") > 50, \"Alerte\")\n",
    "    .when((col(\"humidity\") < 10) & (col(\"temperature\") > 40), \"Critique\")\n",
    "    .when((col(\"vibration\") > 5) & ((col(\"pressure\") < 950) | (col(\"pressure\") > 1050)), \"Alerte\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Capteurs inactifs (si aucune donnée depuis >1h) => traitement par une autre logique (batch/join avec état)\n",
    "\n",
    "# Exemple de corrélation régionale : moyenne température par région (arrondi lat/lon à 0.1°)\n",
    "regional_alerts = alert_df \\\n",
    "    .withColumn(\"region_lat\", expr(\"round(latitude, 1)\")) \\\n",
    "    .withColumn(\"region_lon\", expr(\"round(longitude, 1)\")) \\\n",
    "    .groupBy(window(\"timestamp\", \"10 minutes\"), \"region_lat\", \"region_lon\") \\\n",
    "    .avg(\"temperature\", \"humidity\") \\\n",
    "    .withColumnRenamed(\"avg(temperature)\", \"avg_temp\") \\\n",
    "    .withColumnRenamed(\"avg(humidity)\", \"avg_humidity\") \\\n",
    "    .withColumn(\"regional_status\", when(col(\"avg_temp\") > 45, \"Alerte regionale\").otherwise(\"OK\"))\n",
    "\n",
    "query1 = alert_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"/notebooks/iot_output_json\") \\\n",
    "    .option(\"checkpointLocation\", \"/notebooks/iot_checkpoint_json\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Écriture dans Elasticsearch\n",
    "query2 = alert_df.writeStream \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_iot\") \\\n",
    "    .option(\"es.nodes\", \"es01\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "    .option(\"es.net.ssl\", \"true\") \\\n",
    "    .option(\"es.net.ssl.cert.allow.self.signed\", \"true\") \\\n",
    "    .option(\"es.net.http.auth.user\", \"elastic\") \\\n",
    "    .option(\"es.net.http.auth.pass\", \"Eselpil2\") \\\n",
    "    .option(\"es.resource\", \"agritech_iot_data_enriched\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Kafka stream visible côté console (debug)\n",
    "query3 = raw_df.selectExpr(\"CAST(value AS STRING)\").writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "query1.awaitTermination()\n",
    "query2.awaitTermination()\n",
    "query3.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
