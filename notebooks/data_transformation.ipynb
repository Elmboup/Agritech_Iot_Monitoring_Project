{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d5c777-a887-4bc7-bade-61ce06b7a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import col, when, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Data Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration de la source de données (ici on imagine une source Kafka)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b966047-818b-4f15-a2b4-3acc1f6f563c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47117a0e-bfeb-404f-a630-b263d4018309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifier la configuration de Kafka\n",
    "kafka_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"iot_data_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Les données de Kafka sont stockées dans des colonnes 'key' et 'value' sous forme de 'binary'.\n",
    "# Nous devons décoder les données et les transformer en un DataFrame structuré.\n",
    "\n",
    "# Définir un schéma pour les données IoT\n",
    "schema = StructType([\n",
    "    StructField(\"machine_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"vibration\", FloatType(), True),\n",
    "])\n",
    "\n",
    "# Décoder les données et les structurer\n",
    "iot_data_df = kafka_stream_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Affichage des premières lignes\n",
    "iot_data_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ece63-a63e-4085-b328-99d06b518a05",
   "metadata": {},
   "source": [
    "## Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26f0fc-ea61-484d-983c-db2106e61c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes où des valeurs critiques sont manquantes (par exemple, température ou vibration)\n",
    "iot_data_clean_df = iot_data_df.dropna(subset=[\"temperature\", \"vibration\"])\n",
    "\n",
    "# Affichage après nettoyage\n",
    "iot_data_clean_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be334d14-08d8-4d52-809a-bfaa251a7269",
   "metadata": {},
   "source": [
    "## calcul de statut des machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2acff0-d081-4f1a-9d74-03a4e253850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir des seuils pour les statuts des machines\n",
    "iot_data_clean_df = iot_data_clean_df.withColumn(\n",
    "    \"machine_status\",\n",
    "    when((col(\"temperature\") > 40) | (col(\"vibration\") > 1000), \"Critique\")\n",
    "    .when((col(\"temperature\") > 35) & (col(\"temperature\") <= 40), \"Alerte\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "iot_data_clean_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9419922-6bc1-421f-a218-e09abfefbd4d",
   "metadata": {},
   "source": [
    "## Calcul de la durée de l'état actuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c7364-4175-4c52-8dcf-8a861df0caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir le timestamp en type datetime pour pouvoir calculer la durée\n",
    "iot_data_clean_df = iot_data_clean_df.withColumn(\n",
    "    \"timestamp\", unix_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Calculer la durée en minutes entre chaque événement (fenêtre temporelle)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "windowSpec = Window.partitionBy(\"machine_id\").orderBy(\"timestamp\")\n",
    "\n",
    "iot_data_clean_df = iot_data_clean_df.withColumn(\n",
    "    \"duration_in_current_status\",\n",
    "    (F.unix_timestamp(\"timestamp\") - F.unix_timestamp(F.lag(\"timestamp\", 1).over(windowSpec))) / 60\n",
    ")\n",
    "\n",
    "iot_data_clean_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fbd27-4a72-4bd1-afa6-d8e0dc6a55d1",
   "metadata": {},
   "source": [
    "## Détection des anomalies et ajout de métriques supplémentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed91c5-54f1-4418-966f-3cb1bac07a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalie basée sur une température excessive\n",
    "iot_data_clean_df = iot_data_clean_df.withColumn(\n",
    "    \"is_anomaly\",\n",
    "    when((col(\"temperature\") > 40) | (col(\"vibration\") > 1000), True).otherwise(False)\n",
    ")\n",
    "\n",
    "iot_data_clean_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeaed3d-5432-403f-a299-52e63decfa1d",
   "metadata": {},
   "source": [
    "## Envoi des données transformées vers Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e138590-94e5-42c4-87b6-2b6934480211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'index Elasticsearch\n",
    "es_config = {\n",
    "    \"es.nodes\": \"localhost\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.index.auto.create\": \"true\",\n",
    "    \"es.write.operation\": \"index\"\n",
    "}\n",
    "\n",
    "# Envoi des données transformées vers Elasticsearch en streaming\n",
    "iot_data_clean_df.writeStream \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint/\") \\\n",
    "    .options(**es_config) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63615817-a531-4e46-adc3-7e4938ccee4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[NON_TIME_WINDOW_NOT_SUPPORTED_IN_STREAMING] Window function is not supported in MAX(TIMESTAMP_SEC#875L) (as column `_we0`) on streaming DataFrames/Datasets. Structured Streaming only supports time-window aggregation using the WINDOW function. (window specification: (PARTITION BY MACHINE_ID ORDER BY TIMESTAMP_SEC ASC NULLS FIRST RANGE BETWEEN -600L FOLLOWING AND CURRENT ROW))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 116\u001b[0m\n\u001b[1;32m     93\u001b[0m iot_df \u001b[38;5;241m=\u001b[39m iot_df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatut_global\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m     when((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malerte_sol_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCritique\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatut_capteur\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDéfaillant\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCritique\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Écriture vers Elasticsearch\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[43miot_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.elasticsearch.spark.sql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/checkpoint_iot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.nodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.port\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m9200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.nodes.wan.only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.net.ssl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.net.ssl.cert.allow.self.signed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.net.http.auth.user\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melastic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.net.http.auth.pass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEselpil2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.resource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miot_data_enriched/doc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.mapping.id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munique_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m--> 116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NON_TIME_WINDOW_NOT_SUPPORTED_IN_STREAMING] Window function is not supported in MAX(TIMESTAMP_SEC#875L) (as column `_we0`) on streaming DataFrames/Datasets. Structured Streaming only supports time-window aggregation using the WINDOW function. (window specification: (PARTITION BY MACHINE_ID ORDER BY TIMESTAMP_SEC ASC NULLS FIRST RANGE BETWEEN -600L FOLLOWING AND CURRENT ROW))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, from_json, unix_timestamp, lag, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Initialiser Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoTDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# 2. Définir le schéma des données JSON venant de Kafka\n",
    "schema = StructType() \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"machine_id\", StringType()) \\\n",
    "    .add(\"region\", StringType()) \\\n",
    "    .add(\"season\", StringType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"humidity\", DoubleType()) \\\n",
    "    .add(\"soil_moisture\", DoubleType()) \\\n",
    "    .add(\"vibration\", DoubleType()) \\\n",
    "    .add(\"pressure\", DoubleType())\n",
    "\n",
    "# 3. Lire les données Kafka\n",
    "raw_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\n",
    "    .option(\"subscribe\", \"iot_raw_data\") \\\n",
    "    .load()\n",
    "\n",
    "json_df = raw_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "# Assurez-vous que timestamp est bien en TimestampType\n",
    "iot_df = json_df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Crée une version numérique du timestamp\n",
    "iot_df = iot_df.withColumn(\"timestamp_sec\", unix_timestamp(\"timestamp\"))\n",
    "\n",
    "# Ajout des colonnes d’alerte\n",
    "iot_df = iot_df.withColumn(\n",
    "    \"alerte_temperature\", \n",
    "    when(col(\"temperature\") > 50, \"Alerte\").otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "iot_df = iot_df.withColumn(\n",
    "    \"alerte_sol_sec\",\n",
    "    when((col(\"humidity\") < 10) & (col(\"temperature\") > 40), \"Critique\").otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "iot_df = iot_df.withColumn(\n",
    "    \"alerte_vibration_pression\",\n",
    "    when((col(\"vibration\") > 5) & (col(\"pressure\") > 1.5), \"Alerte\").otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# ⚠️ Utiliser une fenêtre basée sur timestamp_sec (BIGINT), pas sur timestamp (TIMESTAMP)\n",
    "windowSpec = Window \\\n",
    "    .partitionBy(\"machine_id\") \\\n",
    "    .orderBy(\"timestamp_sec\") \\\n",
    "    .rangeBetween(-600, 0)  # 10 minutes en secondes\n",
    "\n",
    "# Calcul de délai de lecture : max timestamp_sec dans la fenêtre\n",
    "iot_df = iot_df.withColumn(\n",
    "    \"delai_lecture\", \n",
    "    col(\"timestamp_sec\") - F.max(\"timestamp_sec\").over(windowSpec)\n",
    ")\n",
    "\n",
    "iot_df = iot_df.withColumn(\n",
    "    \"statut_capteur\",\n",
    "    when(col(\"delai_lecture\") > 3600, \"Défaillant\").otherwise(\"Actif\")\n",
    ")\n",
    "\n",
    "# Alerte régionale\n",
    "alerte_regionale_df = iot_df \\\n",
    "    .filter((col(\"alerte_temperature\") == \"Alerte\") | (col(\"alerte_vibration_pression\") == \"Alerte\")) \\\n",
    "    .groupBy(\"region\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") >= 2) \\\n",
    "    .withColumn(\"alerte_regionale\", F.lit(\"Alerte régionale\")) \\\n",
    "    .select(\"region\", \"alerte_regionale\")\n",
    "\n",
    "iot_df = iot_df.join(alerte_regionale_df, on=\"region\", how=\"left\") \\\n",
    "    .fillna({\"alerte_regionale\": \"Rien à signaler\"})\n",
    "\n",
    "# Statut global\n",
    "iot_df = iot_df.withColumn(\n",
    "    \"statut_global\",\n",
    "    when((col(\"alerte_sol_sec\") == \"Critique\") | (col(\"statut_capteur\") == \"Défaillant\"), \"Critique\")\n",
    "    .when((col(\"alerte_temperature\") == \"Alerte\") |\n",
    "          (col(\"alerte_vibration_pression\") == \"Alerte\") |\n",
    "          (col(\"alerte_regionale\") == \"Alerte régionale\"), \"Alerte\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Écriture vers Elasticsearch\n",
    "iot_df.writeStream \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_iot\") \\\n",
    "    .option(\"es.nodes\", \"es01\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "    .option(\"es.net.ssl\", \"true\") \\\n",
    "    .option(\"es.net.ssl.cert.allow.self.signed\", \"true\") \\\n",
    "    .option(\"es.net.http.auth.user\", \"elastic\") \\\n",
    "    .option(\"es.net.http.auth.pass\", \"Eselpil2\") \\\n",
    "    .option(\"es.resource\", \"iot_data_enriched/doc\") \\\n",
    "    .option(\"es.mapping.id\", \"unique_id\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aaea3ac-c044-406b-b136-02471bcd5c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = b1ee4349-6340-40ee-8496-a7b155b5d48c, runId = 645f9f1a-6539-449e-ad88-101775df5776] terminated with exception: Failed to create new KafkaAdminClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 4. Écrire dans Elasticsearch (index iot_simplifie)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m query \u001b[38;5;241m=\u001b[39m json_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.elasticsearch.spark.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/checkpoint_iot_simple\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes.nodes.wan.only\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 44\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = b1ee4349-6340-40ee-8496-a7b155b5d48c, runId = 645f9f1a-6539-449e-ad88-101775df5776] terminated with exception: Failed to create new KafkaAdminClient"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# 1. Définir le schéma\n",
    "schema = StructType() \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"machine_id\", StringType()) \\\n",
    "    .add(\"region\", StringType()) \\\n",
    "    .add(\"season\", StringType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"humidity\", DoubleType()) \\\n",
    "    .add(\"soil_moisture\", DoubleType()) \\\n",
    "    .add(\"vibration\", DoubleType()) \\\n",
    "    .add(\"pressure\", DoubleType())\n",
    "\n",
    "# 2. Lire depuis Kafka\n",
    "raw_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host.docker.internal:9092\") \\\n",
    "    .option(\"subscribe\", \"iot_raw_data\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# 3. Convertir la colonne `value` (bytes) en string, puis JSON\n",
    "json_df = raw_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# 4. Écrire dans Elasticsearch (index iot_simplifie)\n",
    "query = json_df.writeStream \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_iot_simple\") \\\n",
    "    .option(\"es.nodes\", \"es01\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "    .option(\"es.net.ssl\", \"true\") \\\n",
    "    .option(\"es.net.ssl.cert.allow.self.signed\", \"true\") \\\n",
    "    .option(\"es.net.http.auth.user\", \"elastic\") \\\n",
    "    .option(\"es.net.http.auth.pass\", \"Eselpil2\") \\\n",
    "    .option(\"es.resource\", \"iot_simplifie/_doc\") \\\n",
    "    .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
