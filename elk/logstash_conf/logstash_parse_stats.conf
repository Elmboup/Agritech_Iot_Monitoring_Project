input {

file {

path => "/usr/share/logstash/ingest_data/data_parse_stats.csv"
start_position => "beginning"
sincedb_path => "/dev/null"

}

}

filter {
  csv {
    separator => ","
    columns => ["creation_time", "insertday", "inserthour", "tablename", "source", "count_split", "count_filter", "count_parsed", "count_errors", "count_rows", "count_df_initial", "count_df_enriched", "count_df_distinct", "year", "month", "day", "hour"]
  }
  mutate {
    convert => {
      "count_split" => "integer"
      "count_filter" => "integer"
      "count_parsed" => "integer"
      "count_errors" => "integer"
      "count_rows" => "integer"
      "count_df_initial" => "integer"
      "count_df_enriched" => "integer"
      "count_df_distinct" => "integer"
      "year" => "integer"
      "month" => "integer"
      "day" => "integer"
      "hour" => "integer"
    }
  }
  date {
    match => ["creation_time", "ISO8601"]
  }
}
output {
  elasticsearch {
    hosts => "https://es01:9200"
    index => "data_parse_stats"
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    ssl_enabled => true
    ssl_verification_mode => "none"
  }

}


